{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f186fdf",
   "metadata": {},
   "source": [
    "# Role 3 Evaluation Notebook\n",
    "\n",
    "This notebook assumes **you have a file named `predictions.csv` in the same folder as this notebook**.\n",
    "\n",
    "It will:\n",
    "- Load `predictions.csv`\n",
    "- Detect the actual and prediction columns\n",
    "- Compute RMSE, MAE, MAPE, R², and Directional Accuracy (DA)\n",
    "- Create comparison tables and visualizations.\n",
    "\n",
    "> **Usage:** Place this notebook and `predictions.csv` in the same directory, then run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997c97d",
   "metadata": {},
   "source": [
    "## Install dependencies from `requirements.txt` (optional)\n",
    "\n",
    "If you have a `requirements.txt` file in the **same folder** as this notebook, you can run the cell below\n",
    "to install all required packages into your current environment.\n",
    "\n",
    "> If everything is already installed (for example, you’re using a pre-configured virtual environment),\n",
    "> you can skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies listed in requirements.txt\n",
    "# Make sure requirements.txt is in the same folder as this notebook.\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf14a9f",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80756941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957dbf8",
   "metadata": {},
   "source": [
    "## 2. Load `predictions.csv`\n",
    "\n",
    "This version assumes `predictions.csv` is in the **same folder** as this notebook.\n",
    "If it's somewhere else, update the path in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path below if your file is not in the same folder.\n",
    "df = pd.read_csv('predictions.csv')\n",
    "\n",
    "print('Columns found in predictions.csv:')\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c20fbb",
   "metadata": {},
   "source": [
    "## 3. Identify Actual, Date, and Prediction Columns\n",
    "\n",
    "This cell automatically:\n",
    "- Detects the **actual price** column (searching for `Actual`, `Close`, or `Price` in the name).\n",
    "- Detects a **Date** column if present.\n",
    "- Treats all remaining columns as **model prediction** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect actual/true price column (case-insensitive)\n",
    "actual_col = None\n",
    "for c in df.columns:\n",
    "    name = c.lower()\n",
    "    if 'actual' in name or 'close' in name or 'price' in name:\n",
    "        actual_col = c\n",
    "        break\n",
    "\n",
    "if actual_col is None:\n",
    "    raise ValueError(\n",
    "        'Could not automatically find the actual price column. '\n",
    "        'Rename the true-price column to include \"Actual\", \"Close\", or \"Price\".'\n",
    "    )\n",
    "\n",
    "# Detect date column if present\n",
    "date_col = None\n",
    "for c in df.columns:\n",
    "    if 'date' in c.lower():\n",
    "        date_col = c\n",
    "        break\n",
    "\n",
    "# Prediction columns = all columns except actual + date\n",
    "exclude_cols = [actual_col]\n",
    "if date_col is not None:\n",
    "    exclude_cols.append(date_col)\n",
    "\n",
    "model_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "print(f'Detected actual price column: {actual_col}')\n",
    "if date_col:\n",
    "    print(f'Detected date column: {date_col}')\n",
    "print('Detected model prediction columns:', model_cols)\n",
    "\n",
    "# Parse date column if present\n",
    "if date_col is not None:\n",
    "    try:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    except Exception as e:\n",
    "        print('Warning: could not parse Date column as datetime:', e)\n",
    "\n",
    "y_true = df[actual_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a5798",
   "metadata": {},
   "source": [
    "## 4. Define Metric Functions\n",
    "\n",
    "We define helper functions for:\n",
    "- **Directional Accuracy (DA)** – fraction of times the model gets the direction of change correct.\n",
    "- **MAPE** – Mean Absolute Percentage Error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"Directional Accuracy: fraction of times the model gets the direction of change right.\"\"\"\n",
    "    true_diff = y_true.diff()\n",
    "    pred_diff = y_pred.diff()\n",
    "\n",
    "    # Drop the first NaN caused by diff()\n",
    "    true_sign = np.sign(true_diff.iloc[1:])\n",
    "    pred_sign = np.sign(pred_diff.iloc[1:])\n",
    "\n",
    "    # Align indices\n",
    "    pred_sign = pred_sign.reindex(true_sign.index)\n",
    "\n",
    "    matches = (true_sign == pred_sign)\n",
    "    return matches.mean()\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"MAPE: Mean Absolute Percentage Error. Returns a value between 0 and 1.\"\"\"\n",
    "    y_true_arr = np.array(y_true)\n",
    "    y_pred_arr = np.array(y_pred)\n",
    "    eps = 1e-10  # avoid division by zero\n",
    "    return np.mean(np.abs((y_true_arr - y_pred_arr) / (y_true_arr + eps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041a9a7",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics for Each Model\n",
    "\n",
    "For each prediction column, we compute:\n",
    "- RMSE\n",
    "- MAE\n",
    "- MAPE\n",
    "- R²\n",
    "- DA\n",
    "\n",
    "We then build a comparison table and a \"pretty\" version with rounded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for col in model_cols:\n",
    "    y_pred = df[col]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    da   = directional_accuracy(y_true, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        'Model': col,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'R2':  r2,\n",
    "        'DA':  da\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df = metrics_df.sort_values(by='RMSE').reset_index(drop=True)\n",
    "\n",
    "print('=== Final Metrics Table (raw) ===')\n",
    "display(metrics_df)\n",
    "\n",
    "# Pretty version for reporting\n",
    "metrics_pretty = metrics_df.copy()\n",
    "metrics_pretty['RMSE'] = metrics_pretty['RMSE'].round(2)\n",
    "metrics_pretty['MAE']  = metrics_pretty['MAE'].round(2)\n",
    "metrics_pretty['MAPE'] = (metrics_pretty['MAPE'] * 100).round(2)  # %\n",
    "metrics_pretty['R2']   = metrics_pretty['R2'].round(3)\n",
    "metrics_pretty['DA']   = (metrics_pretty['DA'] * 100).round(2)    # %\n",
    "\n",
    "print('=== Final Metrics Table (pretty, % for MAPE & DA) ===')\n",
    "display(metrics_pretty)\n",
    "\n",
    "# Save raw metrics to CSV\n",
    "metrics_df.to_csv('metrics_summary.csv', index=False)\n",
    "print('Saved metrics_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9d345",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "This section creates:\n",
    "- Actual vs Predicted plot (all models)\n",
    "- RMSE bar chart\n",
    "- MAE bar chart\n",
    "- MAPE bar chart\n",
    "- R² bar chart\n",
    "- Directional Accuracy bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bde0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis for plots\n",
    "if 'date_col' in globals() and date_col is not None:\n",
    "    x_vals = df[date_col]\n",
    "    x_label = 'Date'\n",
    "else:\n",
    "    x_vals = np.arange(len(df))\n",
    "    x_label = 'Index'\n",
    "\n",
    "# A. Actual vs Predicted (all models)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_vals, y_true, label='Actual', linewidth=2)\n",
    "\n",
    "for col in model_cols:\n",
    "    plt.plot(x_vals, df[col], label=col)\n",
    "\n",
    "plt.title('Actual vs Predicted Bitcoin Price')\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# B. RMSE Bar Chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Model'], metrics_df['RMSE'])\n",
    "plt.title('RMSE by Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# C. MAE Bar Chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Model'], metrics_df['MAE'])\n",
    "plt.title('MAE by Model')\n",
    "plt.ylabel('MAE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# D. MAPE Bar Chart (%)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Model'], metrics_df['MAPE'] * 100)\n",
    "plt.title('MAPE by Model')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# E. R² Bar Chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Model'], metrics_df['R2'])\n",
    "plt.title('R² by Model')\n",
    "plt.ylabel('R²')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# F. Directional Accuracy Bar Chart (%)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Model'], metrics_df['DA'] * 100)\n",
    "plt.title('Directional Accuracy by Model')\n",
    "plt.ylabel('DA (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921eaf7",
   "metadata": {},
   "source": [
    "## 7. Optional: Error Distributions\n",
    "\n",
    "These histograms show the distribution of errors (Actual - Predicted) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in model_cols:\n",
    "    errors = y_true - df[col]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(errors, bins=40)\n",
    "    plt.title(f'Error Distribution - {col}')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
